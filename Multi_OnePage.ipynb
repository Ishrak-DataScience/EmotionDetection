{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORpDuI4N+okGHcASzQq9bl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ishrak-DataScience/EmotionDetection/blob/main/Multi_OnePage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#Now you can use the uploaded file, for example:\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/eng.csv') #where fn is the name of your file\n",
        "print(df.head())\n",
        "\n",
        "#!pip install transformers\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Multilingual BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=5)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# Train the model\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}\")\n",
        "\n",
        "    val_report = eval_model(model, val_loader, device)\n",
        "    print(f\"Validation Report:\\n{val_report}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'emotion_model_multilingual.pth')\n",
        "\n"
      ],
      "metadata": {
        "id": "rf_2HWmV4_4m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}